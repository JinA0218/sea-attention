{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import cm\n",
    "from PIL import Image\n",
    "\n",
    "ZOOM = 1\n",
    "\n",
    "def convert_to_colormap(arr: np.ndarray):\n",
    "    T, T = arr.shape\n",
    "    arr_min, arr_max = np.min(arr), np.max(arr)\n",
    "    normalized = (arr - arr_min) / (arr_max - arr_min + 1e-12)\n",
    "    colormapped = cm.gist_earth(normalized)\n",
    "    gamma = 0.2\n",
    "    colormapped = (colormapped / np.max(colormapped)) ** gamma\n",
    "    im = Image.fromarray((colormapped*255).astype(np.uint8))\n",
    "    arr = np.asarray(im)[:, :, :3]\n",
    "    arr = cv2.resize(arr, None, fx=ZOOM, fy=ZOOM, interpolation=cv2.INTER_NEAREST)\n",
    "    border = np.ones((arr.shape[0]+2, arr.shape[1]+2, arr.shape[2]), dtype=np.uint8)\n",
    "    border = border * 255\n",
    "    border[1:-1, 1:-1, :] = arr\n",
    "    return border"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize one tensor\n",
    "import os\n",
    "os.makedirs(f\"./plots/poc/grid_sampling/\", exist_ok=True)\n",
    "\n",
    "def visualize_one_tensor(tensor, name):\n",
    "    img = convert_to_colormap(tensor.numpy())\n",
    "    path = f\"./plots/poc/grid_sampling/{name}.png\"\n",
    "    cv2.imwrite(path, img)\n",
    "    print('processed', path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# a = (torch.rand((1, 1, 203, 128)) ).float() # > 0.5\n",
    "\n",
    "# visualize_one_tensor(a[0][0],'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def grid_sample_bf16(input, grid, mode='nearest', align_corners=False, padding_mode='zeros'):\n",
    "    input_dtype = input.dtype\n",
    "    op_dtype = torch.float32 if torch.get_autocast_gpu_dtype() == torch.bfloat16 else input_dtype\n",
    "    if op_dtype != input_dtype:\n",
    "        input = input.to(op_dtype)\n",
    "        grid = grid.to(op_dtype)\n",
    "    y = F.grid_sample(\n",
    "        input=input,\n",
    "        grid=grid,\n",
    "        mode=mode,\n",
    "        align_corners=align_corners,\n",
    "        padding_mode='zeros',\n",
    "    )\n",
    "    if y.dtype != input_dtype:\n",
    "        y = y.to(input_dtype)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = torch.ones(1, 1, 1, 203)\n",
    "attention_mask[10:]= 0\n",
    "zero_one_attention_mask = (attention_mask > -1).float()\n",
    "\n",
    "\n",
    "def resize_from_m_to_t(x, masked_fill_value, target_width=None):\n",
    "    N, H, T1, T_M = x.shape\n",
    "    if target_width is not None:\n",
    "        T2 = target_width\n",
    "    else:\n",
    "        T2 = T1\n",
    "# with timer(\"resize\"):\n",
    "    # with timer(\"resize.grid\"):\n",
    "    if not False:\n",
    "        token_index_x = zero_one_attention_mask.view(N, 1, T2)\n",
    "        if masked_fill_value is not None:\n",
    "            # token_index_x = torch.roll(token_index_x, shifts=(1,), dims=(-1)).cumsum(-1) + ((1.0 - zero_one_attention_mask) * 2).view(N, 1, T2)\n",
    "            # token_index_x = (token_index_x / ((zero_one_attention_mask.sum(-1) + 2).view(N, 1, 1) + 1e-8) * 2 - 1).expand(N, T1, T2)\n",
    "            mask = token_index_x\n",
    "            mask_cs = mask.cumsum(-1)\n",
    "            token_length = (mask_cs[:, :, -1].unsqueeze(-1) - 1) + 3 * (mask_cs[:, :, -1].unsqueeze(-1)/T_M)\n",
    "            token_index_x = torch.clamp(((((mask_cs - 1) + (1 - mask) * 5000)) / (token_length + 1e-8)) * 2 - 1, -1, 1)\n",
    "            token_index_x = token_index_x.expand(N, T1, T2)\n",
    "        else:\n",
    "            token_index_x = token_index_x.cumsum(-1)\n",
    "            token_index_x = (token_index_x / ((zero_one_attention_mask.sum(-1) - 1).view(N, 1, 1) + 1e-8) * 2 - 1).expand(N, T1, T2)\n",
    "    else:\n",
    "        assert masked_fill_value is not None\n",
    "        mask = (causal_attention_mask > -1).float()\n",
    "        _N, _H, _TQ, _TK = mask.shape\n",
    "        mask_cs = mask.cumsum(-1)\n",
    "        token_length = (mask_cs[:, :, :, -1].unsqueeze(-1) - 1) + 3 * (_TK/T_M)\n",
    "        token_index_x = torch.clamp((((mask_cs - 1) + (1 - mask) * (5000  * (_TK/T_M))) / (token_length + 1e-8)) * 2 - 1, -1, 1)\n",
    "        assert _H == 1\n",
    "        token_index_x = token_index_x[:,0,:,:]\n",
    "    token_index_y = (\n",
    "        torch.arange(T1, dtype=torch.long, device=token_index_x.device)\\\n",
    "            .view(1, T1, 1) / T1 * 2 - 1)\\\n",
    "            .expand(N, T1, T2) #type: torch.Tensor\n",
    "    token_index = torch.cat([\n",
    "        token_index_x.unsqueeze(-1),\n",
    "        token_index_y.unsqueeze(-1)\n",
    "    ], dim=-1)\n",
    "\n",
    "# with timer(\"resize.sample\"):\n",
    "    grid_input = F.pad(F.pad(x, pad=(0, 2), value=0), pad=(0, 1), value=masked_fill_value) if masked_fill_value is not None else x\n",
    "    if grid_input.dtype != x.dtype:\n",
    "        grid_input = grid_input.to(x.dtype)\n",
    "    if token_index.dtype != x.dtype:\n",
    "        token_index = token_index.to(x.dtype)\n",
    "    \n",
    "    return grid_sample_bf16(\n",
    "        input=grid_input,\n",
    "        grid=token_index,\n",
    "        mode='nearest',\n",
    "        align_corners=True,\n",
    "        padding_mode='border'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b = resize_from_m_to_t(a, masked_fill_value=0)\n",
    "\n",
    "# visualize_one_tensor(b[0][0],'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25984"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "203*128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 25984])\n",
      "torch.Size([1, 1, 25000])\n",
      "processed ./plots/poc/grid_sampling/a.png\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = (torch.rand((1, 1, 203, 128)) ).float().view(1,1,203*128) # > 0.5\n",
    "a_inx = torch.topk(input=a, k =25000, dim=-1)\n",
    "print(a.shape)\n",
    "print(a_inx[1].shape)\n",
    "a.scatter_(dim=-1, index = a_inx[1], value=0)\n",
    "a = a.view(1,1,203,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed ./plots/poc/grid_sampling/a.png\n",
      "processed ./plots/poc/grid_sampling/a_l.png\n"
     ]
    }
   ],
   "source": [
    "# 203 * 128\n",
    "\n",
    "inx = torch.tensor([3, 6, 30, 47, 67, 125]).view(1,1,1,6).expand(1, 1, 203, 6)\n",
    "a_l = torch.scatter(a, dim=-1, index=inx, value=1)\n",
    "\n",
    "visualize_one_tensor(a[0][0],'a')\n",
    "visualize_one_tensor(a_l[0][0],'a_l')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed ./plots/poc/grid_sampling/b.png\n",
      "processed ./plots/poc/grid_sampling/b_l.png\n"
     ]
    }
   ],
   "source": [
    "b = resize_from_m_to_t(a , masked_fill_value=0) # * (attention_mask.transpose(-1,-2)>0)\n",
    "b_l = resize_from_m_to_t(a_l, masked_fill_value=0)\n",
    "\n",
    "visualize_one_tensor(b[0][0],'b')\n",
    "visualize_one_tensor(b_l[0][0],'b_l')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 203, 128])\n",
      "torch.Size([1, 1, 203, 128])\n",
      "torch.Size([1, 1, 203, 203])\n",
      "torch.Size([1, 1, 203, 203])\n"
     ]
    }
   ],
   "source": [
    "print(a.shape)\n",
    "print(a_l.shape)\n",
    "\n",
    "print(b.shape)\n",
    "print(b_l.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_a_l = a_l.sum(dim=-2)\n",
    "(sum_a_l==203).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_b_l = b_l.sum(dim=-2)\n",
    "(sum_b_l==203).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.515625"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6*(203/128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "partial attention mask\n",
    "실제로는 주로 -inf으로 채워지고 일부만 0 값으로 채워진 n^2 matrix에\n",
    "특정 column들만 다 0으로 채워줌\n",
    "그 후에 resize_from_m_to_t\n",
    "\n",
    "resize_from_m_to_t 결과 다 0으로 채워진 column이 사라지지는 않는가? 그 두께는?\n",
    ">> 지금 어떤 것은 thick하게 변했고, 어떤 것은 안 변함\n",
    ">> 이거는 grid sample의 mechanism을 이해해야 넘어갈 수 있을듯...\n",
    ">> 또 attention mask까지 고려된 \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 25984])\n",
      "torch.Size([1, 1, 900])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = (torch.rand((1, 1, 203, 128)) ).float().view(1,1,203*128) # > 0.5\n",
    "a_inx = torch.topk(input=a, k =900, dim=-1)\n",
    "print(a.shape)\n",
    "print(a_inx[1].shape)\n",
    "a.fill_(-1000)\n",
    "a.scatter_(dim=-1, index = a_inx[1], value=0)\n",
    "\n",
    "a = a.view(1,1,203,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(900)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1000., -1000., -1000.,  ..., -1000., -1000., -1000.],\n",
       "          [-1000., -1000., -1000.,  ..., -1000., -1000., -1000.],\n",
       "          [-1000., -1000., -1000.,  ..., -1000., -1000., -1000.],\n",
       "          ...,\n",
       "          [-1000., -1000., -1000.,  ..., -1000., -1000., -1000.],\n",
       "          [-1000., -1000., -1000.,  ..., -1000., -1000., -1000.],\n",
       "          [-1000., -1000., -1000.,  ..., -1000., -1000., -1000.]]]])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print((a==0).sum())\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed ./plots/poc/grid_sampling/a.png\n",
      "processed ./plots/poc/grid_sampling/a_l.png\n"
     ]
    }
   ],
   "source": [
    "# 203 * 128\n",
    "\n",
    "inx = torch.tensor([3, 6, 30, 47, 67, 125]).view(1,1,1,6).expand(1, 1, 203, 6)\n",
    "a_l = torch.scatter(a, dim=-1, index=inx, value=0)\n",
    "\n",
    "visualize_one_tensor(a[0][0],'a')\n",
    "visualize_one_tensor(a_l[0][0],'a_l')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed ./plots/poc/grid_sampling/b.png\n",
      "processed ./plots/poc/grid_sampling/b_l.png\n"
     ]
    }
   ],
   "source": [
    "b = resize_from_m_to_t(a , masked_fill_value=0) # * (attention_mask.transpose(-1,-2)>0)\n",
    "b_l = resize_from_m_to_t(a_l, masked_fill_value=0)\n",
    "\n",
    "visualize_one_tensor(b[0][0],'b')\n",
    "visualize_one_tensor(b_l[0][0],'b_l')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6)\n",
      "tensor(11)\n"
     ]
    }
   ],
   "source": [
    "print((a_l.sum(dim=-2)==0).sum())\n",
    "print((b_l.sum(dim=-2)==0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[175], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m \u001b[39mimport\u001b[39;00m transforms\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPIL\u001b[39;00m \u001b[39mimport\u001b[39;00m Image\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPIL\u001b[39;00m \u001b[39mimport\u001b[39;00m ImageFile\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "im_path = '11.jpg'\n",
    "image = Image.open(im_path)\n",
    "image = image.convert('RGB')\n",
    "\n",
    "im_transform = transforms.Compose([\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "image = im_transform(image)\n",
    "# Add batch dimension\n",
    "image = image.unsqueeze(dim=0)\n",
    "\n",
    "d = torch.linspace(-1, 1, 224)\n",
    "meshx, meshy = torch.meshgrid((d, d))\n",
    "\n",
    "# Just to see the effect\n",
    "meshx = meshx * 0.3\n",
    "meshy = meshy * 0.9\n",
    "\n",
    "grid = torch.stack((meshy, meshx), 2)\n",
    "grid = grid.unsqueeze(0)\n",
    "warped = F.grid_sample(image, grid, mode='bilinear', align_corners=False)\n",
    "\n",
    "to_image = transforms.ToPILImage()\n",
    "to_image(image.squeeze()).show()\n",
    "to_image(warped.squeeze()).show(title='WARPED')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
