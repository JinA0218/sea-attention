{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import os\n",
    "import argparse\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from typing import List, Tuple, Optional\n",
    "from matplotlib import cm\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/d1/JINA_COLSELkim/anaconda3/envs/torch1/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "module_path = \"/d1/jinakim/permutation-learning/src/\"\n",
    "sys.path.append(module_path)\n",
    "\n",
    "from utils import batch_to\n",
    "from models import hf_bert as bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_TO_VALID = {\n",
    "    \"cola\": \"validation\",\n",
    "    \"mnli\": \"validation_matched\",\n",
    "    \"mrpc\": \"test\",\n",
    "    \"qnli\": \"validation\",\n",
    "    \"qqp\": \"validation\",\n",
    "    \"rte\": \"validation\",\n",
    "    \"sst2\": \"validation\",\n",
    "    \"stsb\": \"validation\",\n",
    "    \"wnli\": \"validation\",\n",
    "    \"bert\": \"validation\",\n",
    "}\n",
    "\n",
    "TASK_TO_KEYS = {\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"qqp\": (\"question1\", \"question2\"),\n",
    "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import hf_bert as berts\n",
    "import transformers\n",
    "\n",
    "def get_base_model(dataset, only_tokenizer=False):\n",
    "    checkpoint = {\n",
    "        \"cola\": \"textattack/bert-base-uncased-CoLA\",\n",
    "        \"mnli\": \"yoshitomo-matsubara/bert-base-uncased-mnli\",\n",
    "        \"mrpc\": \"textattack/bert-base-uncased-MRPC\",\n",
    "        # \"mrpc\": \"M-FAC/bert-tiny-finetuned-mrpc\",\n",
    "        \"qnli\": \"textattack/bert-base-uncased-QNLI\",\n",
    "        \"qqp\": \"textattack/bert-base-uncased-QQP\",\n",
    "        \"rte\": \"textattack/bert-base-uncased-RTE\",\n",
    "        \"sst2\": \"textattack/bert-base-uncased-SST-2\",\n",
    "        \"stsb\": \"textattack/bert-base-uncased-STS-B\",\n",
    "        \"wnli\": \"textattack/bert-base-uncased-WNLI\",\n",
    "        \"bert\": \"bert-base-uncased\",\n",
    "    }[dataset]\n",
    "\n",
    "    # NOTE(HJ): this bert models has special hooks\n",
    "    model = {\n",
    "        \"cola\": berts.BertForSequenceClassification,\n",
    "        \"mnli\": berts.BertForSequenceClassification,\n",
    "        \"mrpc\": berts.BertForSequenceClassification,\n",
    "        \"qnli\": berts.BertForSequenceClassification,\n",
    "        \"qqp\": berts.BertForSequenceClassification,\n",
    "        \"rte\": berts.BertForSequenceClassification,\n",
    "        \"sst2\": berts.BertForSequenceClassification,\n",
    "        \"stsb\": berts.BertForSequenceClassification,\n",
    "        \"wnli\": berts.BertForSequenceClassification,\n",
    "        \"bert\": berts.BertForSequenceClassification,\n",
    "    }[dataset]\n",
    "    \n",
    "    tokenizer = transformers.BertTokenizerFast.from_pretrained(checkpoint)\n",
    "    if only_tokenizer:\n",
    "        return None, tokenizer\n",
    "    \n",
    "    bert = model.from_pretrained(checkpoint, cache_dir='./cache/huggingface/')\n",
    "    return bert, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "import random, torch\n",
    "\n",
    "def get_dataloader(subset, tokenizer, batch_size, split='train', encode_batch_size=384):\n",
    "    if subset == 'bert':\n",
    "        subset = \"cola\" #return dummy set\n",
    "    \n",
    "    dataset = load_dataset('glue', subset, split=split, cache_dir='./cache/datasets')\n",
    "    \n",
    "    sentence1_key, sentence2_key = TASK_TO_KEYS[subset]\n",
    "\n",
    "    def encode(examples):\n",
    "        # Tokenize the texts\n",
    "        args = (\n",
    "            (examples[sentence1_key],) if sentence2_key is None else (examples[sentence1_key], examples[sentence2_key])\n",
    "        )\n",
    "        result = tokenizer(*args, padding=True, max_length=256, truncation=True)\n",
    "        # result = tokenizer(*args, padding=\"max_length\", max_length=512, truncation=True)\n",
    "        # Map labels to IDs (not necessary for GLUE tasks)\n",
    "        # if label_to_id is not None and \"label\" in examples:\n",
    "        #     result[\"label\"] = [(label_to_id[l] if l != -1 else -1) for l in examples[\"label\"]]\n",
    "        return result\n",
    "    \n",
    "    if split.startswith('train'): #shuffle when train set\n",
    "        dataset = dataset.sort('label')\n",
    "        dataset = dataset.shuffle(seed=random.randint(0, 10000))\n",
    "    dataset = dataset.map(lambda examples: {'labels': examples['label']}, batched=True, batch_size=encode_batch_size)\n",
    "    dataset = dataset.map(encode, batched=True, batch_size=encode_batch_size)\n",
    "    dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset, \n",
    "        batch_size=batch_size, \n",
    "        num_workers=0,\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_fixed_batch(dataloader: DataLoader, batch_size: int):\n",
    "    items = [\n",
    "        dataloader.dataset.__getitem__(i * (len(dataloader.dataset) // batch_size))\n",
    "        for i in range(batch_size)\n",
    "    ]\n",
    "    max_len = max([it['input_ids'].shape[0] for it in items])\n",
    "    for it in items:\n",
    "        it['input_ids'] = F.pad(it['input_ids'], (0, max_len-len(it['input_ids'])))\n",
    "        it['attention_mask'] = F.pad(it['attention_mask'], (0, max_len-len(it['attention_mask'])))\n",
    "        it['token_type_ids'] = F.pad(it['token_type_ids'], (0, max_len-len(it['token_type_ids'])))\n",
    "    # print([[(k, v.shape) for k, v in it.items()] for it in items])\n",
    "    return dataloader.collate_fn(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/d1/JINA_COLSELkim/permutation-learning/src/poc/cat/cache/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Loading cached processed dataset at /d1/JINA_COLSELkim/permutation-learning/src/poc/cat/cache/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-e435b23c87ec8670.arrow\n",
      "Loading cached processed dataset at /d1/JINA_COLSELkim/permutation-learning/src/poc/cat/cache/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-7d8b16f65baf29cc.arrow\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "BF16 = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "\n",
    "N, H, T, T_M =16, 12, 203, 128\n",
    "subset = \"mnli\"\n",
    "\n",
    "base_model, tokenizer = get_base_model(subset)\n",
    "base_model.to(device=0)\n",
    "\n",
    "loader = get_dataloader(subset, tokenizer, N, split=TASK_TO_VALID[subset])\n",
    "batch = gather_fixed_batch(loader, N)\n",
    "batch = batch_to(batch, device=0)\n",
    "\n",
    "base_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    base_model(**batch)\n",
    "\n",
    "for module in base_model.modules():\n",
    "    if isinstance(module, bert.BertSelfAttention):\n",
    "        teacher_score = module.perlin_last_attention_score\n",
    "        teacher_probs = module.perlin_last_attention_prob\n",
    "        teacher_context_layer = module.perlin_last_context_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'teacher_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m teacher_score\n",
      "\u001b[0;31mNameError\u001b[0m: name 'teacher_score' is not defined"
     ]
    }
   ],
   "source": [
    "teacher_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
